# Paper&Code
Record my research and encourage myself

* (arXiv 2022.03) Transformer-based Multimodal Information Fusion for Facial Expression Analysis, 【[Paper](https://arxiv.org/pdf/2203.12367.pdf)】
* (arXiv 2022.03) Facial Expression Recognition with Swin Transformer,【[Paper](https://arxiv.org/pdf/2203.13472.pdf)】
* (arXiv 2022.01) Training Vision Transformers with Only 2040 Images,【[Paper](https://arxiv.org/pdf/2201.10728.pdf)】
* (arXiv 2021.09) Sparse Spatial Transformers for Few-Shot Learning, 【[Paper](https://arxiv.org/pdf/2109.10057.pdf)】,【[Code](https://github.com/chenhaoxing/SSFormers)】
* (arXiv 2021.06)How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers,【[Paper](https://arxiv.org/pdf/2106.10270.pdf)】 
* (arXiv 2021.03) Vision Transformers for Dense Prediction,【[Paper](https://arxiv.org/pdf/2103.13413.pdf)】,【[Code](https://github.com/isl-org/DPT)】
* (arXiv 2021.03) Face Transformer for Recognition, 【[Paper](https://arxiv.org/pdf/2103.14803.pdf)】
* (arXiv 2021.03) Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, 【[Paper](https://arxiv.org/pdf/2103.14030.pdf)】,【[Code](https://github.com/microsoft/Swin-Transformer)】
* (ICLR'21) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, 【[Paper](https://arxiv.org/pdf/2010.11929.pdf)】,【[Code](https://github.com/google-research/vision_transformer)】
* (arXiv 2017.06v1)Attention Is All You Need, 【[Paper](https://arxiv.org/pdf/1706.03762.pdf)】
